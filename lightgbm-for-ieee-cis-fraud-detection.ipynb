{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-09T07:12:19.068991Z",
     "iopub.status.busy": "2025-12-09T07:12:19.068760Z",
     "iopub.status.idle": "2025-12-09T07:12:30.279963Z",
     "shell.execute_reply": "2025-12-09T07:12:30.278965Z",
     "shell.execute_reply.started": "2025-12-09T07:12:19.068969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T02:59:27.522326Z",
     "iopub.status.busy": "2025-12-03T02:59:27.521953Z",
     "iopub.status.idle": "2025-12-03T02:59:27.527718Z",
     "shell.execute_reply": "2025-12-03T02:59:27.526624Z",
     "shell.execute_reply.started": "2025-12-03T02:59:27.522298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade scikit-learn==1.7.2\n",
    "# import sklearn\n",
    "# print(sklearn.__version__)\n",
    "# !pip install --upgrade lightgbm\n",
    "print(lgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:13:07.093447Z",
     "iopub.status.busy": "2025-12-09T07:13:07.093136Z",
     "iopub.status.idle": "2025-12-09T07:13:37.710786Z",
     "shell.execute_reply": "2025-12-09T07:13:37.709967Z",
     "shell.execute_reply.started": "2025-12-09T07:13:07.093424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Auto-detect Kaggle environment\n",
    "if os.path.exists(\"/kaggle/input\"):\n",
    "    TRAIN_TRANS = \"/kaggle/input/ieee-fraud-detection/train_transaction.csv\"\n",
    "    TRAIN_IDENT = \"/kaggle/input/ieee-fraud-detection/train_identity.csv\"\n",
    "else:\n",
    "    TRAIN_TRANS = \"train_transaction.csv\"\n",
    "    TRAIN_IDENT = \"train_identity.csv\"\n",
    "\n",
    "train_tr = pd.read_csv(TRAIN_TRANS, index_col=\"TransactionID\")\n",
    "train_id = pd.read_csv(TRAIN_IDENT, index_col=\"TransactionID\")\n",
    "\n",
    "train = train_tr.merge(train_id, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "# target\n",
    "y = train[\"isFraud\"]\n",
    "X = train.drop(\"isFraud\", axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "#  Feature list\n",
    "# ---------------------------\n",
    "new_features = [\n",
    "            \"TransactionID\",\n",
    "            \"TransactionAmt\",\n",
    "            \"DeviceInfo\",\n",
    "            \"TransactionDT\",\n",
    "            \"ProductCD\",\n",
    "            \"card1\",\n",
    "            \"card2\",\n",
    "            \"card3\",\n",
    "            \"card4\",\n",
    "            \"card5\",\n",
    "            \"card6\",\n",
    "            \"addr1\",\n",
    "            \"addr2\",\n",
    "            \"dist1\",\n",
    "            \"dist2\",\n",
    "            \"P_emaildomain\",\n",
    "            \"R_emaildomain\",\n",
    "        ]\n",
    "# ---------------------------\n",
    "#  Use these features for better performance and above for reasonable and fast performance\n",
    "# ---------------------------\n",
    "# new_features = [\n",
    "#     'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card5', 'card6',\n",
    "#     'addr1', 'addr2', 'dist1', 'dist2',\n",
    "#     'P_emaildomain', 'R_emaildomain',\n",
    "#     'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "#     'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15',\n",
    "#     'M1', 'M2', 'M3', 'M4', 'M6', 'M7', 'M8', 'M9',\n",
    "#     'V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', 'V23', 'V26', \n",
    "#     'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56',\n",
    "#     'V59', 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86',\n",
    "#     'V88', 'V89', 'V91', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121',\n",
    "#     'V123', 'V124', 'V127', 'V129', 'V130', 'V136', 'V138', 'V139', 'V142',\n",
    "#     'V147', 'V156', 'V160', 'V162', 'V165', 'V166', 'V169', 'V171', 'V173',\n",
    "#     'V175', 'V176', 'V178', 'V180', 'V182', 'V185', 'V187', 'V188', 'V198',\n",
    "#     'V203', 'V205', 'V207', 'V209', 'V210', 'V215', 'V218', 'V220', 'V221',\n",
    "#     'V223', 'V224', 'V226', 'V228', 'V229', 'V234', 'V235', 'V238', 'V240',\n",
    "#     'V250', 'V252', 'V253', 'V257', 'V258', 'V260', 'V261', 'V264', 'V266',\n",
    "#     'V267', 'V271', 'V274', 'V277', 'V281', 'V283', 'V284', 'V285', 'V286',\n",
    "#     'V289', 'V291', 'V294', 'V296', 'V297', 'V301', 'V303', 'V305', 'V307',\n",
    "#     'V309', 'V310', 'V314', 'V320',\n",
    "#     'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10',\n",
    "#     'id_11', 'id_12', 'id_13', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19',\n",
    "#     'id_20', 'id_28', 'id_29', 'id_31', 'id_35', 'id_36', 'id_37', 'id_38',\n",
    "#     'DeviceType', 'DeviceInfo'\n",
    "# ]\n",
    "# Filter only available columns\n",
    "features = [f for f in new_features if f in X.columns]\n",
    "X = X[features]\n",
    "\n",
    "# ---------------------------\n",
    "# Fill NA\n",
    "# ---------------------------\n",
    "X = X.fillna(-999)\n",
    "\n",
    "# ---------------------------\n",
    "# Label Encoding\n",
    "# ---------------------------\n",
    "label_encoders = {}\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "joblib.dump(label_encoders, \"label_encoders.pkl\")\n",
    "joblib.dump(features, \"model_features.pkl\")\n",
    "\n",
    "# ---------------------------\n",
    "# Train/Val Split\n",
    "# ---------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# LightGBM Model\n",
    "# ---------------------------\n",
    "train_ds = lgb.Dataset(X_train, label=y_train)\n",
    "val_ds = lgb.Dataset(X_val, label=y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:22:32.693927Z",
     "iopub.status.busy": "2025-12-09T07:22:32.692935Z",
     "iopub.status.idle": "2025-12-09T07:29:04.387339Z",
     "shell.execute_reply": "2025-12-09T07:29:04.386160Z",
     "shell.execute_reply.started": "2025-12-09T07:22:32.693893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttraining's auc: 0.94068\tvalid_1's auc: 0.913912\n",
      "[400]\ttraining's auc: 0.966935\tvalid_1's auc: 0.931471\n",
      "[600]\ttraining's auc: 0.980516\tvalid_1's auc: 0.941028\n",
      "[800]\ttraining's auc: 0.987849\tvalid_1's auc: 0.945804\n",
      "[1000]\ttraining's auc: 0.992008\tvalid_1's auc: 0.949551\n",
      "[1200]\ttraining's auc: 0.995008\tvalid_1's auc: 0.952272\n",
      "[1400]\ttraining's auc: 0.996736\tvalid_1's auc: 0.954013\n",
      "[1600]\ttraining's auc: 0.997865\tvalid_1's auc: 0.955349\n",
      "[1800]\ttraining's auc: 0.9986\tvalid_1's auc: 0.956509\n",
      "[2000]\ttraining's auc: 0.99907\tvalid_1's auc: 0.9572\n",
      "[2200]\ttraining's auc: 0.999396\tvalid_1's auc: 0.957837\n",
      "[2400]\ttraining's auc: 0.999603\tvalid_1's auc: 0.958292\n",
      "[2600]\ttraining's auc: 0.999749\tvalid_1's auc: 0.958649\n",
      "[2800]\ttraining's auc: 0.999835\tvalid_1's auc: 0.958938\n",
      "[3000]\ttraining's auc: 0.999892\tvalid_1's auc: 0.959222\n",
      "[3200]\ttraining's auc: 0.99993\tvalid_1's auc: 0.959506\n",
      "[3400]\ttraining's auc: 0.999957\tvalid_1's auc: 0.959692\n",
      "[3600]\ttraining's auc: 0.999974\tvalid_1's auc: 0.959906\n",
      "[3800]\ttraining's auc: 0.999983\tvalid_1's auc: 0.960058\n",
      "[4000]\ttraining's auc: 0.99999\tvalid_1's auc: 0.960176\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3996]\ttraining's auc: 0.99999\tvalid_1's auc: 0.960177\n",
      "Validation AUC: 0.9601774263093422\n",
      "MODEL TRAINED AND SAVED SUCCESSFULLY!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/786179626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MODEL TRAINED AND SAVED SUCCESSFULLY!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tree_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tree_structure\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tree {i}: depth = {depth}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_depth'"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 128,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_ds,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    num_boost_round=4000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=300),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model.save_model(\"ieee_lgbm.txt\")\n",
    "# Evaluate Validation AUC\n",
    "val_pred = model.predict(X_val)\n",
    "auc = roc_auc_score(y_val, val_pred)\n",
    "print(\"Validation AUC:\", auc)\n",
    "print(\"MODEL TRAINED AND SAVED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:42:19.533045Z",
     "iopub.status.busy": "2025-12-09T07:42:19.532713Z",
     "iopub.status.idle": "2025-12-09T07:42:35.209828Z",
     "shell.execute_reply": "2025-12-09T07:42:35.209009Z",
     "shell.execute_reply.started": "2025-12-09T07:42:19.533021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Depth Summary ---\n",
      "Max depth: 41\n",
      "Min depth: 13\n",
      "Average depth: 20.97\n"
     ]
    }
   ],
   "source": [
    "def get_tree_depth(tree_structure):\n",
    "    if \"left_child\" not in tree_structure and \"right_child\" not in tree_structure:\n",
    "        return 1\n",
    "\n",
    "    left_depth = right_depth = 0\n",
    "\n",
    "    if \"left_child\" in tree_structure:\n",
    "        left_depth = get_tree_depth(tree_structure[\"left_child\"])\n",
    "    if \"right_child\" in tree_structure:\n",
    "        right_depth = get_tree_depth(tree_structure[\"right_child\"])\n",
    "\n",
    "    return 1 + max(left_depth, right_depth)\n",
    "\n",
    "\n",
    "# Collect depths\n",
    "model_json = model.dump_model()\n",
    "tree_depths = []\n",
    "\n",
    "for tree in model_json[\"tree_info\"]:\n",
    "    depth = get_tree_depth(tree[\"tree_structure\"])\n",
    "    tree_depths.append(depth)\n",
    "\n",
    "# Statistics\n",
    "max_depth = max(tree_depths)\n",
    "min_depth = min(tree_depths)\n",
    "avg_depth = sum(tree_depths) / len(tree_depths)\n",
    "\n",
    "print(\"\\n--- Depth Summary ---\")\n",
    "print(f\"Max depth: {max_depth}\")\n",
    "print(f\"Min depth: {min_depth}\")\n",
    "print(f\"Average depth: {avg_depth:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T05:17:23.973150Z",
     "iopub.status.busy": "2025-12-03T05:17:23.972606Z",
     "iopub.status.idle": "2025-12-03T05:20:19.955903Z",
     "shell.execute_reply": "2025-12-03T05:20:19.954469Z",
     "shell.execute_reply.started": "2025-12-03T05:17:23.973119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Auto-detect Kaggle\n",
    "if os.path.exists(\"/kaggle/input\"):\n",
    "    TEST_TRANS = \"/kaggle/input/ieee-fraud-detection/test_transaction.csv\"\n",
    "    TEST_IDENT = \"/kaggle/input/ieee-fraud-detection/test_identity.csv\"\n",
    "else:\n",
    "    TEST_TRANS = \"test_transaction.csv\"\n",
    "    TEST_IDENT = \"test_identity.csv\"\n",
    "\n",
    "# Load test data\n",
    "test_tr = pd.read_csv(TEST_TRANS, index_col=\"TransactionID\")\n",
    "test_id = pd.read_csv(TEST_IDENT, index_col=\"TransactionID\")\n",
    "test = test_tr.merge(test_id, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "MODEL_PATH = \"/kaggle/working/ieee_lgbm.txt\"\n",
    "FEATURES_PATH = \"/kaggle/working/model_features.pkl\"\n",
    "ENCODERS_PATH = \"/kaggle/working/label_encoders.pkl\"\n",
    "\n",
    "model = lgb.Booster(model_file=MODEL_PATH)\n",
    "features = joblib.load(FEATURES_PATH)\n",
    "label_encoders = joblib.load(ENCODERS_PATH)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Choose only a subset of features for testing\n",
    "# -----------------------------\n",
    "selected_features = [\n",
    "            \"TransactionID\",\n",
    "            \"TransactionAmt\",\n",
    "            \"DeviceInfo\",\n",
    "            \"TransactionDT\",\n",
    "            \"ProductCD\",\n",
    "            \"card1\",\n",
    "            \"card2\",\n",
    "            \"card3\",\n",
    "            \"card4\",\n",
    "            \"card5\",\n",
    "            \"card6\",\n",
    "            \"addr1\",\n",
    "            \"addr2\",\n",
    "            \"dist1\",\n",
    "            \"dist2\",\n",
    "            \"P_emaildomain\",\n",
    "            \"R_emaildomain\",\n",
    "        ]\n",
    "\n",
    "# Keep only features that exist in test\n",
    "existing_features = [f for f in selected_features if f in test.columns]\n",
    "X_test = test[existing_features].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocess categorical features\n",
    "# -----------------------------\n",
    "for col in X_test.columns:\n",
    "    if col in label_encoders:\n",
    "        le = label_encoders[col]\n",
    "        # Replace unseen labels with a placeholder\n",
    "        X_test[col] = X_test[col].astype(str).map(lambda x: x if x in le.classes_ else \"unseen_before_label\")\n",
    "        # Refit the encoder to include the placeholder if missing\n",
    "        if \"unseen_before_label\" not in le.classes_:\n",
    "            le_classes = list(le.classes_) + [\"unseen_before_label\"]\n",
    "            le.classes_ = np.array(le_classes)\n",
    "        X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# -----------------------------\n",
    "# Fill missing numeric features with -999\n",
    "# -----------------------------\n",
    "for col in X_test.columns:\n",
    "    if col not in label_encoders:  # numeric\n",
    "        X_test[col] = X_test[col].fillna(-999)\n",
    "\n",
    "# -----------------------------\n",
    "# Make predictions\n",
    "# -----------------------------\n",
    "preds = model.predict(X_test)\n",
    "auc = roc_auc_score(y_val, preds)\n",
    "# submission = pd.DataFrame({\n",
    "#     \"TransactionID\": X_test.index,\n",
    "#     \"isFraud\": preds\n",
    "# })\n",
    "\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Saved submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T10:04:06.081525Z",
     "iopub.status.busy": "2025-12-08T10:04:06.081123Z",
     "iopub.status.idle": "2025-12-08T10:09:01.375010Z",
     "shell.execute_reply": "2025-12-08T10:09:01.374197Z",
     "shell.execute_reply.started": "2025-12-08T10:04:06.081489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Auto-detect Kaggle environment\n",
    "if os.path.exists(\"/kaggle/input\"):\n",
    "    TRAIN_TRANS = \"/kaggle/input/ieee-fraud-detection/train_transaction.csv\"\n",
    "    TRAIN_IDENT = \"/kaggle/input/ieee-fraud-detection/train_identity.csv\"\n",
    "else:\n",
    "    TRAIN_TRANS = \"train_transaction.csv\"\n",
    "    TRAIN_IDENT = \"train_identity.csv\"\n",
    "\n",
    "train_tr = pd.read_csv(TRAIN_TRANS, index_col=\"TransactionID\")\n",
    "train_id = pd.read_csv(TRAIN_IDENT, index_col=\"TransactionID\")\n",
    "\n",
    "train = train_tr.merge(train_id, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "# target\n",
    "y = train[\"isFraud\"]\n",
    "X = train.drop(\"isFraud\", axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "#  Feature list\n",
    "# ---------------------------\n",
    "new_features = [\n",
    "    \"TransactionID\", \"TransactionAmt\", \"DeviceInfo\", \"TransactionDT\",\n",
    "    \"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\",\n",
    "    \"addr1\", \"addr2\", \"dist1\", \"dist2\", \"P_emaildomain\", \"R_emaildomain\"\n",
    "]\n",
    "features = [f for f in new_features if f in X.columns]\n",
    "X = X[features]\n",
    "\n",
    "# ---------------------------\n",
    "# Fill NA\n",
    "# ---------------------------\n",
    "X = X.fillna(-999)\n",
    "\n",
    "# ---------------------------\n",
    "# Label Encoding\n",
    "# ---------------------------\n",
    "label_encoders = {}\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "joblib.dump(label_encoders, \"label_encoders.pkl\")\n",
    "joblib.dump(features, \"model_features.pkl\")\n",
    "\n",
    "# ---------------------------\n",
    "# Train/Val Split\n",
    "# ---------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# LightGBM Model\n",
    "# ---------------------------\n",
    "train_ds = lgb.Dataset(X_train, label=y_train)\n",
    "val_ds = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "evals_result = {}  # dictionary to store evaluation results\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 128,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 3,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_ds,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=3000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=200),\n",
    "        lgb.log_evaluation(period=200),\n",
    "        lgb.record_evaluation(evals_result)  # <-- store metrics here\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save_model(\"ieee_lgbm.txt\")\n",
    "joblib.dump(evals_result, \"evals_result.pkl\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate Validation AUC\n",
    "# ---------------------------\n",
    "val_pred = model.predict(X_val)\n",
    "auc = roc_auc_score(y_val, val_pred)\n",
    "print(\"Validation AUC:\", auc)\n",
    "\n",
    "# ============================================================\n",
    "# 1. TRAINING vs VALIDATION AUC CURVE\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(evals_result[\"train\"][\"auc\"], label=\"Train AUC\")\n",
    "plt.plot(evals_result[\"valid\"][\"auc\"], label=\"Validation AUC\")\n",
    "plt.title(\"AUC Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"curve_auc.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 2. TRAINING vs VALIDATION LOGLOSS CURVE\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(evals_result[\"train\"][\"binary_logloss\"], label=\"Train Logloss\")\n",
    "plt.plot(evals_result[\"valid\"][\"binary_logloss\"], label=\"Valid Logloss\")\n",
    "plt.title(\"Logloss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Logloss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"curve_logloss.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 3. FEATURE IMPORTANCE\n",
    "# ============================================================\n",
    "importance = model.feature_importance(importance_type=\"gain\")\n",
    "indices = np.argsort(importance)[::-1]\n",
    "feat_names_sorted = X.columns[indices]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(feat_names_sorted, importance[indices])\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 4. PRECISIONâ€“RECALL CURVE\n",
    "# ============================================================\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_pred)\n",
    "ap = average_precision_score(y_val, val_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall, precision)\n",
    "plt.title(f\"Precision-Recall Curve (AP = {ap:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"precision_recall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"ALL PLOTS SAVED!\")\n",
    "print(\" - curve_auc.png\")\n",
    "print(\" - curve_logloss.png\")\n",
    "print(\" - feature_importance.png\")\n",
    "print(\" - precision_recall.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 568274,
     "isSourceIdPinned": false,
     "sourceId": 14242,
     "sourceType": "competition"
    },
    {
     "datasetId": 8905112,
     "sourceId": 13969004,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
